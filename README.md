# SnakeRL

项目的核心原理是 深度 Q 网络（Deep Q-Network, DQN），它是将深度学习（Deep Learning）与强化学习（Reinforcement Learning）结合的经典算法。

1. 核心概念：Q-Learning
强化学习的本质是试错学习。在贪吃蛇中，智能体（Agent）不知道什么动作是好的，它只知道环境（Environment）给它的奖励（Reward）。

Q 值 (Quality Value)
Q(s, a) 代表在状态 s 下，采取动作 a，未来能获得的累计总奖励（不仅仅是这一步）。

如果 Q(s, 直行) = 10，Q(s, 左转) = 5
那么聪明的 AI 就会选择直行。
核心目标：训练一个“大脑”，给它任何状态 s，它能准确告诉你每个动作的 Q 值。

2. 深度神经网络扮演的角色
在传统的 Q-Learning 中，我们用一张巨大的表格记录所有可能状态的 Q 值（Q-Table）。但在贪吃蛇中，状态太多了（蛇的位置千变万化），表格存不下。

我们要用深度神经网络（DQN）来“拟合”这个表格：

输入层：11 个神经元（对应我们的状态向量：危险检测、方向、食物位置）
隐藏层：256 个神经元（负责提取特征，理解复杂的战局）
输出层：3 个神经元（分别输出直行、左转、右转的 Q 值预测）
它就像一个直觉极强的专家，看一眼棋盘（状态），就能告诉你这三步棋哪步胜率最高（Q 值）。

3. 训练过程：如何让它变聪明？
训练是一个不断自我对弈、收集经验、反思修正的过程。

步骤一：探索与利用 (ε-greedy)
探索 (Exploration)：有时（概率 ε）乱走一步，尝试未知的路径。万一乱撞发现了新大陆呢？
利用 (Exploitation)：有时按经验（Q 网络预测最大值）走，稳扎稳打。
随着训练进行，ε 从 1 降到 0.01，变得越来越老练。
步骤二：存储经验 (Replay Buffer)
每一次尝试 
(状态 s, 动作 a, 奖励 r, 新状态 s')
 都被存入经验池。 这就像人的回忆。如果不存下来，刚才撞墙的教训马上就忘了。

步骤三：反思学习 (Backpropagation)
AI 定期从“回忆”里随机抽一批经历来反思。它使用贝尔曼方程 (Bellman Equation) 来计算“真实应该得到的 Q 值”：

$$ Q_{\text{target}}(s,a) = \text{奖励} + \gamma \times \max Q(s', a') $$

含义：现在的价值 = 拿到手的好处 + 未来的最大潜力 ($\gamma$ 是折扣因子，代表远见)。
更新网络：如果神经网络预测的 Q 值和算出来的这个目标值偏差很大，就调整网络参数（权重），让下次预测更准。
4. 两个“大脑” (Target Network)
项目中其实有两个网络：

Q 网络 (q_network)：正在学习的学生，负责行动。
目标网络 (target_network)：老师，负责计算标准答案。
如果只有一个网络，它既要行动又要算目标，就像左脚踩右脚上天，很不稳定。 所以我们固定住“老师”一段时间，让学生专门学，过一阵子再把老师更新成最新的学生水平。

总结
整个项目的灵魂就是：

状态抽象：把画面简化成 11 个关键数字。
神经网络：学习这 11 个数字到 3 个动作价值的映射。
经验回放：记住教训，反复复习。
延迟奖励：通过 Q 值公式，学会为了未来的食物而忍受当下的绕路。
这就是为什么它能从一开始的无头苍蝇，变成最后的高瞻远瞩的贪吃蛇高手。
