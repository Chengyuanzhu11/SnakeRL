# 🐍 SnakeRL (贪吃蛇深度强化学习)

这是一个基于 **深度 Q 网络 (DQN)** 和 **PyTorch** 实现的贪吃蛇强化学习项目。它具有现代化的视觉效果和完善的训练记录功能。

![项目演示](training_curve.png)

## ✨ 主要特性

- **🧠 深度强化学习**: 使用三层全连接神经网络学习最优策略。
- **👁️ 状态感知**: 智能体通过 11 维状态向量感知世界（危险探测、移动方向、食物方位）。
- **🎨 现代化视觉**:
  - 流线型蛇身，带有渐变和发光效果
  - 动态跟随的眼睛动画
  - 红苹果风格的食物设计
- **📈 训练可视化**: 实时绘制训练曲线，记录得分和 epsilon 变化。
- **💾 自动存档**: 自动保存/加载最佳模型，支持最高分永久记录。

## 🛠️ 安装指南

1. **克隆项目**
   ```bash
   git clone https://github.com/Chengyuanzhu11/SnakeRL.git
   cd SnakeRL
   ```

2. **安装依赖**
   ```bash
   pip install -r requirements.txt
   ```
   *(依赖库: PyTorch, Pygame/Pygame-CE, NumPy, Matplotlib)*

## 🚀 使用方法

### 🎮 演示模式 (观看 AI 玩游戏)
加载预训练模型，观看 AI 表演：
```bash
python play.py
```
- 可选参数:
  - `--games 5`: 玩 5 局
  - `--model checkpoints/best_model.pth`: 指定模型文件

### 🏋️ 训练模式 (从头训练)
开始训练你自己的智能体：
```bash
python train.py
```
- 可选参数:
  - `--episodes 1000`: 设置训练总回合数
  - `--render`: 开启训练时的画面渲染 (会降低训练速度)

## 🧠 核心原理

智能体使用 **Deep Q-Learning** 算法最大化累积奖励：

1. **状态空间 (11个输入)**:
   - 3个方向的危险检测 (直行、左转、右转)
   - 4个当前的朝向 (上、下、左、右)
   - 4个食物的相对方位

2. **神经网络结构**:
   - 输入层: 11 个神经元
   - 隐藏层: 256 个神经元 (ReLU 激活)
   - 输出层: 3 个动作 (直行、右转、左转)

3. **奖励机制**:
   - **+10**: 吃到食物 🍎
   - **-10**: 死亡 (撞墙/撞自己) 💀
   - **+1/-1**: 靠近/远离食物

## 📊 性能表现

- **100 回合**: 能够学会基本的避障，得分约 5-10 分。
- **500 回合**: 学会高效寻找食物，得分约 20-30 分。
- **1000+ 回合**: 掌握长远策略，极少失误，得分可达 60+。

## 📂 项目结构

```
SnakeRL/
├── agent/              # DQN 智能体与经验回放
├── environment/        # 游戏环境逻辑 (Gym 风格)
├── models/             # PyTorch 神经网络模型
├── utils/              # 辅助工具 (绘图等)
├── train.py            # 训练脚本
├── play.py             # 演示/推理脚本
└── requirements.txt    # 项目依赖
```
